---
description: Go Performance
alwaysApply: false
---

# Go Performance

Go is fast by default. Profile before optimizing — intuition about bottlenecks is usually wrong.

## Profile First

```go
import _ "net/http/pprof"
// go tool pprof http://localhost:6060/debug/pprof/heap
// go build -gcflags="-m" ./...  — escape analysis and inlining
```

## Memory Allocation

```go
// Good: preallocate when length is known
users := make([]User, 0, len(ids))

// Good: sync.Pool for large, frequently allocated buffers
var bufPool = sync.Pool{New: func() any { return new(bytes.Buffer) }}

// Good: strings.Builder for concatenation
var b strings.Builder
for _, s := range parts { b.WriteString(s) }

// Bad: string concat in loop (new alloc every iteration)
for _, s := range parts { result += s }
```

## Pointer vs Value

- Small structs (< ~64 bytes): pass by value — stays on stack
- Large structs or shared state: pass by pointer
- Slices, maps, channels are already references — don't pass pointers to them

## Concurrency Performance

```go
// sync.RWMutex for read-heavy workloads
func (c *Cache) Get(key string) (Entry, bool) {
    c.mu.RLock(); defer c.mu.RUnlock()
    e, ok := c.data[key]; return e, ok
}
// atomic for hot counters
var requestCount atomic.Int64
```

## Database Performance

```go
db.SetMaxOpenConns(25)
db.SetMaxIdleConns(5)
db.SetConnMaxLifetime(5 * time.Minute)
```

## Benchmarking

```go
func BenchmarkJSON(b *testing.B) {
    data := loadTestData()
    b.ResetTimer(); b.ReportAllocs()
    for b.Loop() { json.Unmarshal(data, &result) }
}
// Run: go test -bench=. -benchmem -count=5 ./...
```

## Anti-Patterns

- **Never** optimize without profiling data
- **Never** `sync.Pool` for small, short-lived objects — GC handles those
- **Never** cache without bounds — use LRU with a max size
