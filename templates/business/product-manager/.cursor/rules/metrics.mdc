---
description: Product Metrics
alwaysApply: false
---

# Product Metrics

Guidelines for defining, tracking, and acting on product metrics.

## Metrics Hierarchy

- **North Star Metric**: Single metric capturing core value delivered to customers
- **Input Metrics** (leading): Activation rate, feature adoption, engagement frequency
- **Output Metrics** (lagging): Retention, revenue, customer satisfaction

## North Star Examples by Model

| Model | North Star | Key Inputs |
|-------|------------|------------|
| SaaS | Weekly Active Users | Activation, feature adoption |
| Marketplace | Transactions | Listings, buyer visits |
| E-commerce | Revenue | Traffic, conversion, AOV |
| B2B Platform | Active Accounts | Users per account, API calls |

## OKRs

```text
Objective: [Qualitative, inspiring, time-bound]
├── KR1: [Metric] — Baseline: X → Target: Y
├── KR2: [Metric] — Baseline: X → Target: Y
└── KR3: [Metric] — Baseline: X → Target: Y
```

- 3-5 Objectives per quarter, 2-4 KRs each
- 70% achievement = success (stretch goals)
- Outcomes, not outputs: "Reduce churn to 5%" not "Launch retention feature"
- 0.7-0.9 score = hit stretch goal; 1.0 = goal was too easy

## AARRR (Pirate Metrics)

| Stage | Question | Example Metrics |
|-------|----------|-----------------|
| Acquisition | How do users find us? | Traffic, CAC, channel performance |
| Activation | Great first experience? | Signup rate, onboarding completion |
| Retention | Do they come back? | D1/D7/D30 retention, churn rate |
| Revenue | How do we monetize? | ARPU, LTV, conversion to paid |
| Referral | Do they tell others? | NPS, viral coefficient |

## Key Metric Definitions

| Metric | Formula |
|--------|---------|
| DAU/MAU (stickiness) | Daily active / Monthly active |
| Net Revenue Retention | (Start MRR + Expansion - Churn) / Start MRR |
| LTV:CAC | Customer lifetime value / Acquisition cost (target: 3:1+) |
| Time to Value | Signup to "aha moment" |

## Instrumentation

- Event naming: `object_action` (e.g., `feature_used`, `subscription_upgraded`)
- Required properties: feature_name, user_id, timestamp
- Track: signup, activation, core feature use, upgrade, support contact

## Experimentation

- State hypothesis: "If [change], then [metric] will [improve] because [reason]"
- Define primary metric, guardrails, sample size, duration
- p < 0.05 for standard decisions; p < 0.01 for high-stakes
- No peeking — commit to duration upfront

## Common Pitfalls

- **Vanity metrics**: Tie every metric to a business outcome or remove it from the dashboard.
- **Peeking at experiments**: Pre-calculate sample size and commit. Early stopping inflates false positives.
- **Too many dashboards**: One per audience, 5-8 metrics max.
